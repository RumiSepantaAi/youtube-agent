{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "1kE8w4_C2ynk",
        "outputId": "5e6490cf-03b2-42d1-dbd7-ba8ab2d115e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ OpenAI API-Key verf√ºgbar\n",
            "üöÄ Starte Pipeline\n",
            "\n",
            "‚úÖ Transkript geladen:\n",
            "‚Üí Quelle: transcript_api(auto) | Sprache: de | Segmente: 302 | Zeichen: 11434\n",
            "‚Üí Vorschau: KI-Agenten sind die Zukunft von k√ºnstlicher Intelligenz. Aber was bedeutet das f√ºr dich konkret? Ich habe mich die letzten Jahre intensiv mit LMS auseinandergesetzt und bereits auch schon dazu unterri ‚Ä¶\n",
            "‚Üí Erstes Segment: KI-Agenten sind die Zukunft von ‚Ä¶\n",
            "üóÇ  Debug gespeichert: output/ki-agenten-fur-beginner-alle-grundlagen-in-9-min-einfach-erklart_segments.json | output/ki-agenten-fur-beginner-alle-grundlagen-in-9-min-einfach-erklart_transcript.txt\n",
            "üîé Textvorschau: KI-Agenten sind die Zukunft von k√ºnstlicher Intelligenz. Aber was bedeutet das f√ºr dich konkret? Ich habe mich die letzten Jahre intensiv mit LMS auseinandergesetzt und bereits auch schon dazu unterrichtet. Wenn du jetzt im Job schon regelm√§√üig KI Tools nutzt und wissen willst, wie KI Agenten dich n‚Ä¶\n",
            "\n",
            "üß™ Reiner Text: 11434 Zeichen\n",
            "\n",
            "ü§ñ Verdichtung/Extraktion (LLM ‚Äì Single Shot, JSON only)‚Ä¶\n",
            "\n",
            "‚úÖ Fertig. Datei gespeichert: output/ki-agenten-fur-beginner-alle-grundlagen-in-9-min-einfach-erklart.md\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'output/ki-agenten-fur-beginner-alle-grundlagen-in-9-min-einfach-erklart.md'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# ==============================\n",
        "# Setup\n",
        "# ==============================\n",
        "!pip -q install --upgrade yt-dlp youtube-transcript-api python-slugify openai requests\n",
        "\n",
        "import os, re, json, math, requests, yt_dlp\n",
        "from slugify import slugify\n",
        "from typing import List, Dict, Tuple\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# OpenAI API-Key aus Colab Secrets ODER Env\n",
        "OPENAI_API_KEY = None\n",
        "if userdata:\n",
        "    try:\n",
        "        OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    except Exception:\n",
        "        pass\n",
        "if not OPENAI_API_KEY:\n",
        "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if OPENAI_API_KEY:\n",
        "    OPENAI_API_KEY = OPENAI_API_KEY.strip()\n",
        "    print(\"‚úÖ OpenAI API-Key verf√ºgbar\")\n",
        "else:\n",
        "    raise RuntimeError(\"‚ùå OPENAI_API_KEY fehlt. In Colab unter üîë Secrets setzen oder als Env-Var exportieren.\")\n",
        "\n",
        "# Modell (JSON-Mode-tauglich)\n",
        "MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-5\")\n",
        "\n",
        "# F√ºr 2‚Äì5 Min Videos: kein Chunking\n",
        "USE_CHUNKING = False\n",
        "\n",
        "from youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound, TranscriptsDisabled\n",
        "\n",
        "PREF_LANGS = ['de', 'de-DE', 'de-AT', 'de-CH', 'en', 'en-US', 'en-GB']\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# Utilities: Parsing & Cleaning\n",
        "# ==============================\n",
        "def _ytt_list(video_id: str):\n",
        "    \"\"\"Kompatibel zu alten/neuen youtube-transcript-api-Versionen.\"\"\"\n",
        "    try:\n",
        "        ytt = YouTubeTranscriptApi()\n",
        "        return ytt.list(video_id)\n",
        "    except Exception:\n",
        "        return YouTubeTranscriptApi.list_transcripts(video_id)\n",
        "\n",
        "_UI_META_PAT = re.compile(\n",
        "    r\"^\\s*(?:\\[[^\\]]{0,40}\\]|<[^>]+>|\\([^)]+\\)|\\*{1,3}[^*]{0,40}\\*{1,3})\\s*$|\"\n",
        "    r\"\\s*\\[[^\\]]{2,40}\\]\\s*|\\s*\\([^)]+\\)\\s*|\\s*‚ô™+\\s*|\\s*‚ô´+\\s*\",\n",
        "    re.IGNORECASE\n",
        ")\n",
        "HTML_TAG = re.compile(r\"<[^>]+>\")\n",
        "MULTI_SPACE = re.compile(r\"\\s+\")\n",
        "\n",
        "def _clean_caption_text(s: str) -> str:\n",
        "    \"\"\"Entfernt HTML-Tags, UI-Artefakte (z.B. [Musik], (Applaus)), trimmt Whitespaces.\"\"\"\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    s = HTML_TAG.sub(\"\", s)\n",
        "    # h√§ufige UI-Markups entfernen (de/en)\n",
        "    s = re.sub(r\"\\[(?:music|musik|applause|lachen|ger√§usche|noise|intro|outro)\\]\", \"\", s, flags=re.I)\n",
        "    s = re.sub(r\"\\((?:music|musik|applause|lachen|ger√§usche|noise|intro|outro)\\)\", \"\", s, flags=re.I)\n",
        "    s = s.replace(\"‚ô™\", \"\").replace(\"‚ô´\", \"\")\n",
        "    s = MULTI_SPACE.sub(\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def _parse_ts(ts: str) -> float:\n",
        "    ts = ts.strip().replace(\",\", \".\")\n",
        "    parts = ts.split(\":\")\n",
        "    if len(parts) == 3:\n",
        "        h, m, s = parts\n",
        "    else:\n",
        "        h, m, s = \"0\", parts[0], parts[1]\n",
        "    return int(h)*3600 + int(m)*60 + float(s)\n",
        "\n",
        "def _vtt_to_segments(vtt_text: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    VTT ‚Üí Segmente inkl. Dauer (aus Cue-Abst√§nden).\n",
        "    Entfernt UI-Artefakte & bereinigt Duplikate in kurzer Folge.\n",
        "    \"\"\"\n",
        "    lines = [ln.rstrip(\"\\n\") for ln in vtt_text.splitlines()]\n",
        "    segments = []\n",
        "    cur_start = None\n",
        "    cur_lines = []\n",
        "    last_kept_text = \"\"\n",
        "    last_kept_start = -999.0\n",
        "\n",
        "    for raw in lines:\n",
        "        line = raw.strip()\n",
        "        if not line or line.upper().startswith(\"WEBVTT\") or line.isdigit():\n",
        "            continue\n",
        "        if \"-->\" in line:\n",
        "            # alte Cue flushen\n",
        "            if cur_start is not None:\n",
        "                text = _clean_caption_text(\" \".join(t for t in cur_lines if t and not _UI_META_PAT.match(t)))\n",
        "                text = text.strip()\n",
        "                if text:\n",
        "                    # kurze Duplikate in engem Zeitfenster verwerfen\n",
        "                    if not (text == last_kept_text and (cur_start - last_kept_start) < 1.5):\n",
        "                        segments.append({\"start\": cur_start, \"duration\": 0.0, \"text\": text})\n",
        "                        last_kept_text = text\n",
        "                        last_kept_start = cur_start\n",
        "                cur_lines = []\n",
        "            # neue Cue\n",
        "            cur_start = _parse_ts(line.split(\"-->\")[0])\n",
        "        else:\n",
        "            if not _UI_META_PAT.match(line):\n",
        "                cur_lines.append(line)\n",
        "\n",
        "    if cur_start is not None and cur_lines:\n",
        "        text = _clean_caption_text(\" \".join(t for t in cur_lines if t and not _UI_META_PAT.match(t))).strip()\n",
        "        if text:\n",
        "            if not (text == last_kept_text and (cur_start - last_kept_start) < 1.5):\n",
        "                segments.append({\"start\": cur_start, \"duration\": 0.0, \"text\": text})\n",
        "    # Dauer aus Abst√§nden sch√§tzen\n",
        "    for i in range(len(segments)-1):\n",
        "        segments[i][\"duration\"] = max(0.0, segments[i+1][\"start\"] - segments[i][\"start\"])\n",
        "    return segments\n",
        "\n",
        "def _validate_segments(segs: List[Dict], min_segments=10, min_chars=500) -> Dict:\n",
        "    text = \" \".join(s.get(\"text\",\"\") for s in segs)\n",
        "    ok = bool(segs and len(segs) >= min_segments and len(text) >= min_chars)\n",
        "    preview = (text[:300].replace(\"\\n\",\" \") + \"‚Ä¶\") if text else \"\"\n",
        "    return {\"ok\": ok, \"len_chars\": len(text), \"num_segments\": len(segs), \"preview\": preview}\n",
        "\n",
        "def _pick_track(tracks: Dict) -> Tuple[str, str]:\n",
        "    \"\"\"Bevorzuge .vtt und bevorzugte Sprachen.\"\"\"\n",
        "    if not tracks: return None, None\n",
        "    for lang in PREF_LANGS + list(tracks.keys()):\n",
        "        if lang in tracks:\n",
        "            vtt = next((t for t in tracks[lang] if t.get('ext') == 'vtt' and t.get('url')), None)\n",
        "            if vtt: return vtt['url'], lang\n",
        "            any_t = next((t for t in tracks[lang] if t.get('url')), None)\n",
        "            if any_t: return any_t['url'], lang\n",
        "    return None, None\n",
        "\n",
        "def extract_video_id(url: str) -> str:\n",
        "    pats = [\n",
        "        r'(?:https?:\\/\\/)?(?:www\\.)?youtube\\.com\\/watch\\?v=([a-zA-Z0-9_-]{11})',\n",
        "        r'(?:https?:\\/\\/)?(?:www\\.)?youtu\\.be\\/([a-zA-Z0-9_-]{11})',\n",
        "    ]\n",
        "    for p in pats:\n",
        "        m = re.search(p, url)\n",
        "        if m: return m.group(1)\n",
        "    raise ValueError(f\"Ung√ºltige YouTube-URL: {url}\")\n",
        "\n",
        "def get_video_title(video_id: str) -> str:\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL({'quiet': True, 'no_warnings': True}) as ydl:\n",
        "            info = ydl.extract_info(f\"https://www.youtube.com/watch?v={video_id}\", download=False)\n",
        "        return info.get(\"title\", \"untitled\")\n",
        "    except Exception:\n",
        "        return \"untitled\"\n",
        "\n",
        "def fetch_transcript_segments(youtube_url: str) -> Dict:\n",
        "    \"\"\"\n",
        "    R√ºckgabe:\n",
        "      {success, video_id, title, lang, source, segments, diagnostics}\n",
        "    where segments = [{start, duration, text}, ...]\n",
        "    \"\"\"\n",
        "    try:\n",
        "        vid = extract_video_id(youtube_url)\n",
        "        title = get_video_title(vid)\n",
        "\n",
        "        # 1) youtube-transcript-api: manuell ‚Üí auto\n",
        "        try:\n",
        "            tlist = _ytt_list(vid)\n",
        "            # Zuerst manuell\n",
        "            for code in PREF_LANGS:\n",
        "                try:\n",
        "                    t = tlist.find_manually_created_transcript([code])\n",
        "                    res = t.fetch()\n",
        "                    segs = res.to_raw_data() if hasattr(res, \"to_raw_data\") else res\n",
        "                    # cleanup jedes Segments\n",
        "                    for s in segs:\n",
        "                        s[\"text\"] = _clean_caption_text(s.get(\"text\",\"\"))\n",
        "                    diag = _validate_segments(segs)\n",
        "                    if diag[\"ok\"]:\n",
        "                        diag[\"preview\"] = (\" \".join(s[\"text\"] for s in segs)[:300] + \"‚Ä¶\")\n",
        "                        return {\"success\": True, \"video_id\": vid, \"title\": title, \"lang\": t.language_code,\n",
        "                                \"source\": \"transcript_api(manual)\", \"segments\": segs, \"diagnostics\": diag}\n",
        "                except Exception:\n",
        "                    pass\n",
        "            # Dann auto\n",
        "            for code in PREF_LANGS:\n",
        "                try:\n",
        "                    t = tlist.find_generated_transcript([code])\n",
        "                    res = t.fetch()\n",
        "                    segs = res.to_raw_data() if hasattr(res, \"to_raw_data\") else res\n",
        "                    for s in segs:\n",
        "                        s[\"text\"] = _clean_caption_text(s.get(\"text\",\"\"))\n",
        "                    diag = _validate_segments(segs)\n",
        "                    if diag[\"ok\"]:\n",
        "                        diag[\"preview\"] = (\" \".join(s[\"text\"] for s in segs)[:300] + \"‚Ä¶\")\n",
        "                        return {\"success\": True, \"video_id\": vid, \"title\": title, \"lang\": t.language_code,\n",
        "                                \"source\": \"transcript_api(auto)\", \"segments\": segs, \"diagnostics\": diag}\n",
        "                except Exception:\n",
        "                    pass\n",
        "        except (NoTranscriptFound, TranscriptsDisabled):\n",
        "            pass\n",
        "        except Exception as e:\n",
        "            print(\"transcript-api Fehler:\", e)\n",
        "\n",
        "        # 2) Fallback: yt_dlp ‚Üí VTT\n",
        "        with yt_dlp.YoutubeDL({'quiet': True, 'no_warnings': True}) as ydl:\n",
        "            info = ydl.extract_info(f\"https://www.youtube.com/watch?v={vid}\", download=False)\n",
        "        url, lang = _pick_track(info.get('subtitles') or {})\n",
        "        source = \"ytdlp(subtitles)\" if url else None\n",
        "        if not url:\n",
        "            url, lang = _pick_track(info.get('automatic_captions') or {})\n",
        "            source = \"ytdlp(automatic_captions)\" if url else None\n",
        "        if not url:\n",
        "            return {\"success\": False, \"error\": \"Keine Untertitel/Auto-Captions verf√ºgbar\"}\n",
        "\n",
        "        r = requests.get(url, timeout=20); r.raise_for_status()\n",
        "        segs = _vtt_to_segments(r.text)\n",
        "        diag = _validate_segments(segs)\n",
        "        return {\"success\": diag[\"ok\"], \"video_id\": vid, \"title\": title, \"lang\": lang or \"unknown\",\n",
        "                \"source\": source, \"segments\": segs, \"diagnostics\": diag}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"success\": False, \"error\": str(e)}\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# Text utils\n",
        "# ==============================\n",
        "def segments_to_text(segments: List[Dict]) -> str:\n",
        "    return \" \".join(s.get(\"text\",\"\").strip() for s in segments if s.get(\"text\")).strip()\n",
        "\n",
        "def mmss(seconds: float) -> str:\n",
        "    secs = int(seconds + 0.5)  # runden, nicht abschneiden\n",
        "    m = secs // 60\n",
        "    s = secs % 60\n",
        "    return f\"{m:02d}:{s:02d}\"\n",
        "\n",
        "def show_diagnostics(data: Dict):\n",
        "    if not data.get(\"success\"):\n",
        "        print(\"‚ùå Fehler:\", data.get(\"error\")); return\n",
        "    d = data[\"diagnostics\"]\n",
        "    print(f\"‚Üí Quelle: {data['source']} | Sprache: {data['lang']} | Segmente: {d['num_segments']} | Zeichen: {d['len_chars']}\")\n",
        "    print(\"‚Üí Vorschau:\", d.get(\"preview\",\"\")[:200], \"‚Ä¶\")\n",
        "    print(\"‚Üí Erstes Segment:\", data[\"segments\"][0][\"text\"][:120], \"‚Ä¶\")\n",
        "\n",
        "def save_debug_files(title: str, segments: List[Dict]):\n",
        "    os.makedirs(\"output\", exist_ok=True)\n",
        "    slug = slugify(title) or \"untitled\"\n",
        "    seg_path = f\"output/{slug}_segments.json\"\n",
        "    txt_path = f\"output/{slug}_transcript.txt\"\n",
        "    with open(seg_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"segments\": segments}, f, ensure_ascii=False, indent=2)\n",
        "    text = segments_to_text(segments)\n",
        "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "    print(f\"üóÇ  Debug gespeichert: {seg_path} | {txt_path}\")\n",
        "    print(\"üîé Textvorschau:\", (text[:300] + \"‚Ä¶\") if len(text) > 300 else text)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# OpenAI: Single-shot JSON Call\n",
        "# ==============================\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "def llm_json(prompt: str, model: str = MODEL) -> dict:\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        messages=[\n",
        "            {\"role\":\"system\",\"content\":(\n",
        "                \"Du bist ein pr√§ziser Notiz-Assistent. \"\n",
        "                \"Antworte ausschlie√ülich als valides JSON ‚Äì ohne Markdown, Erkl√§rtexte oder Kommentare.\"\n",
        "            )},\n",
        "            {\"role\":\"user\",\"content\": prompt},\n",
        "        ],\n",
        "    )\n",
        "    return json.loads(resp.choices[0].message.content)\n",
        "\n",
        "def summarize_single_shot(title: str, text: str) -> dict:\n",
        "    \"\"\"\n",
        "    Ein LLM-Aufruf, der alles liefert ‚Äì Zitate NUR als Text (ohne Zeiten).\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "Analysiere das folgende Transkript und gib NUR dieses JSON zur√ºck:\n",
        "{{\n",
        "  \"tldr\": [\"Punkt 1\",\"Punkt 2\",\"Punkt 3\",\"Punkt 4\",\"Punkt 5\"],\n",
        "  \"kernaussagen\": [\"Aussage 1\",\"Aussage 2\",\"Aussage 3\",\"Aussage 4\",\"Aussage 5\",\"Aussage 6\",\"Aussage 7\",\"Aussage 8\"],\n",
        "  \"outline\": [\"1. Abschnitt\",\"2. Abschnitt\",\"3. Abschnitt\",\"4. Abschnitt\",\"5. Abschnitt\",\"6. Abschnitt\"],\n",
        "  \"zitate\": [\"W√∂rtliches Zitat 1\",\"W√∂rtliches Zitat 2\",\"W√∂rtliches Zitat 3\",\"W√∂rtliches Zitat 4\"],\n",
        "  \"glossar\": {{\"Glossar1\", ....}},\n",
        "  \"offene_fragen\": [\"Frage 1\",\"Frage 2\",\"Frage 3,...\"]\n",
        "}}\n",
        "Regeln:\n",
        "- Schreibe deutsch.\n",
        "- Nutze ausschlie√ülich Inhalte aus dem Transkript (keine Halluzinationen).\n",
        "- TL;DR: 3‚Äì5 kurze, konkrete Bullet Points.\n",
        "- Kernaussagen: pr√§zise, faktenbasiert (6‚Äì10 Punkte).\n",
        "- Outline: logisch in 5‚Äì8 Punkten (Stichworte OK).\n",
        "- \"zitate\": maximal 4 **w√∂rtliche und vollst√§ndige** S√§tze aus dem Transkript; KEINE Zeitstempel (die werden sp√§ter gemappt).\n",
        "- Wenn du unsicher bist, lasse das Feld leer [] statt zu raten.\n",
        "\n",
        "Titel: {title}\n",
        "\n",
        "Transkript:\n",
        "{text}\n",
        "\"\"\"\n",
        "    return llm_json(prompt)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# Sentenzbildung + Quote ‚Üí Zeit\n",
        "# ==============================\n",
        "STOP = {\n",
        "    \"der\",\"die\",\"das\",\"und\",\"oder\",\"aber\",\"dass\",\"den\",\"dem\",\"ein\",\"eine\",\"einen\",\"im\",\"in\",\"an\",\"am\",\n",
        "    \"ist\",\"sind\",\"war\",\"waren\",\"mit\",\"zu\",\"auf\",\"f√ºr\",\"von\",\"es\",\"ich\",\"du\",\"wir\",\"ihr\",\"sie\",\"man\",\n",
        "    \"auch\",\"noch\",\"schon\",\"nur\",\"so\",\"wie\",\"wenn\",\"dann\",\"weil\",\"hat\",\"haben\",\"wird\",\"werden\",\"mal\"\n",
        "}\n",
        "TOKEN = re.compile(r\"[a-zA-Z√§√∂√º√Ñ√ñ√ú√ü0-9\\-]+\")\n",
        "\n",
        "def _tokenize(s): return [w.lower() for w in TOKEN.findall(s or \"\")]\n",
        "\n",
        "def join_segments_into_sentences(segments, max_chars=220):\n",
        "    \"\"\"\n",
        "    F√ºhrt Folge-Segmente zu S√§tzen zusammen.\n",
        "    Startzeit = Start des ersten Teilsegments.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    cur_text = []\n",
        "    cur_start = None\n",
        "    seen_norm = set()\n",
        "\n",
        "    def norm(s):\n",
        "        return MULTI_SPACE.sub(\" \", re.sub(r\"[^\\w√§√∂√º√Ñ√ñ√ú√ü-]+\",\" \", (s or \"\").lower())).strip()\n",
        "\n",
        "    for seg in segments:\n",
        "        t = (seg.get(\"text\") or \"\").strip()\n",
        "        if not t:\n",
        "            continue\n",
        "        if cur_start is None:\n",
        "            cur_start = float(seg.get(\"start\", 0.0))\n",
        "        # Silbentrennung grob heilen\n",
        "        if cur_text and cur_text[-1].endswith(\"-\"):\n",
        "            cur_text[-1] = cur_text[-1][:-1] + t\n",
        "        else:\n",
        "            cur_text.append(t)\n",
        "        text_now = \" \".join(cur_text).strip()\n",
        "        if len(text_now) >= max_chars or re.search(r\"[.!?‚Ä¶]\\s*$\", t):\n",
        "            n = norm(text_now)\n",
        "            if n and n not in seen_norm:\n",
        "                sentences.append({\"start\": cur_start, \"text\": text_now})\n",
        "                seen_norm.add(n)\n",
        "            cur_text, cur_start = [], None\n",
        "\n",
        "    if cur_text:\n",
        "        text_now = \" \".join(cur_text).strip()\n",
        "        n = norm(text_now)\n",
        "        if n and n not in seen_norm:\n",
        "            sentences.append({\"start\": cur_start or 0.0, \"text\": text_now})\n",
        "    return sentences\n",
        "\n",
        "def _similarity(a: str, b: str) -> float:\n",
        "    \"\"\"Token-Overlap-Score + Substring-Bonus (robust ohne externe Libs).\"\"\"\n",
        "    ta = [w for w in _tokenize(a) if w not in STOP]\n",
        "    tb = [w for w in _tokenize(b) if w not in STOP]\n",
        "    if not ta or not tb:\n",
        "        return 0.0\n",
        "    sa, sb = set(ta), set(tb)\n",
        "    overlap = len(sa & sb) / max(len(sb), 1)\n",
        "    # Substring-Bonus, wenn Quote (b) vollst√§ndig im Satz (a) steckt\n",
        "    na = \" \".join(ta); nb = \" \".join(tb)\n",
        "    bonus = 0.25 if nb and nb in na else 0.0\n",
        "    return min(1.0, overlap + bonus)\n",
        "\n",
        "def map_quotes_to_times(quotes: List[str], segments: List[Dict], max_quotes=4, min_gap_sec=20.0):\n",
        "    \"\"\"\n",
        "    Mappt Zitat-Texte auf Startzeiten via Satzliste + √Ñhnlichkeit.\n",
        "    \"\"\"\n",
        "    if not quotes:\n",
        "        return []\n",
        "    sentences = join_segments_into_sentences(segments)\n",
        "    # grobe Gesamtdauer (optional f√ºr sp√§tere Heuristiken)\n",
        "    # last = segments[-1] if segments else {\"start\":0,\"duration\":0}\n",
        "    picks = []\n",
        "    used_idx = set()\n",
        "\n",
        "    for q in quotes[:max_quotes]:\n",
        "        q_clean = _clean_caption_text(q).strip()\n",
        "        if not q_clean or len(q_clean.split()) < 6:\n",
        "            continue\n",
        "        # best match\n",
        "        best_i, best_s, best_score = None, None, 0.0\n",
        "        for i, s in enumerate(sentences):\n",
        "            if i in used_idx:\n",
        "                continue\n",
        "            sc = _similarity(s[\"text\"], q_clean)\n",
        "            if sc > best_score:\n",
        "                best_i, best_s, best_score = i, s, sc\n",
        "        # threshold, damit wir keine Knaller daneben w√§hlen\n",
        "        if best_s and best_score >= 0.35:\n",
        "            # Abstand zu bestehenden Picks erzwingen\n",
        "            if any(abs(best_s[\"start\"] - p[\"_start\"]) < min_gap_sec for p in picks):\n",
        "                continue\n",
        "            picks.append({\"text\": best_s[\"text\"], \"time\": mmss(best_s[\"start\"]), \"_start\": best_s[\"start\"]})\n",
        "            used_idx.add(best_i)\n",
        "\n",
        "    for p in picks:\n",
        "        p.pop(\"_start\", None)\n",
        "    return picks\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# Markdown\n",
        "# ==============================\n",
        "def render_markdown(data: Dict, url: str, title: str) -> str:\n",
        "    return f\"\"\"# {title}\n",
        "\n",
        "- Quelle: {url}\n",
        "\n",
        "## TL;DR (3‚Äì5 Bullet Points)\n",
        "{chr(10).join(f\"- {p}\" for p in data.get('tldr', []))}\n",
        "\n",
        "## Kernaussagen\n",
        "{chr(10).join(f\"- {p}\" for p in data.get('kernaussagen', []))}\n",
        "\n",
        "## Struktur / Outline\n",
        "{chr(10).join(f\"{i+1}. {p}\" for i,p in enumerate(data.get('outline', [])))}\n",
        "\n",
        "## Zitate mit Zeitstempel\n",
        "{chr(10).join(f\"- **[{q.get('time','00:00')}]** {q.get('text','')}\" for q in data.get('zitate', []))}\n",
        "\n",
        "## Glossar\n",
        "{chr(10).join(f\"- **{k}**: {v}\" for k,v in (data.get('glossar', {}) or {}).items())}\n",
        "\n",
        "## Offene Fragen\n",
        "{chr(10).join(f\"- {p}\" for p in data.get('offene_fragen', []))}\n",
        "\"\"\".strip()\n",
        "\n",
        "def save_markdown(md_text: str, title: str) -> str:\n",
        "    os.makedirs(\"output\", exist_ok=True)\n",
        "    slug = slugify(title) or \"untitled\"\n",
        "    path = f\"output/{slug}.md\"\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(md_text + \"\\n\")\n",
        "    return path\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# Pipeline\n",
        "# ==============================\n",
        "def run_pipeline(youtube_url: str):\n",
        "    print(\"üöÄ Starte Pipeline\\n\")\n",
        "    data = fetch_transcript_segments(youtube_url)\n",
        "    if not data.get(\"success\"):\n",
        "        print(\"‚ùå Abbruch:\", data.get(\"error\"))\n",
        "        return None\n",
        "\n",
        "    print(\"‚úÖ Transkript geladen:\")\n",
        "    show_diagnostics(data)\n",
        "\n",
        "    # Debug-Dateien ablegen (JSON + Plaintext)\n",
        "    save_debug_files(data[\"title\"], data[\"segments\"])\n",
        "\n",
        "    # Reiner Text\n",
        "    text = segments_to_text(data[\"segments\"])\n",
        "    print(f\"\\nüß™ Reiner Text: {len(text)} Zeichen\")\n",
        "\n",
        "    # Single-Shot Summarization\n",
        "    print(\"\\nü§ñ Verdichtung/Extraktion (LLM ‚Äì Single Shot, JSON only)‚Ä¶\")\n",
        "    summary = summarize_single_shot(data[\"title\"], text)\n",
        "\n",
        "    # Zitate (mit Zeit-Mapping)\n",
        "    # LLM gibt zitate als Liste von Strings zur√ºck. Mappen auf Zeiten.\n",
        "    raw_quotes = summary.get(\"zitate\") or []\n",
        "    if isinstance(raw_quotes, dict):  # falls ein Modell doch Objekte liefert\n",
        "        raw_quotes = list(raw_quotes.values())\n",
        "    raw_quotes = [q if isinstance(q, str) else str(q) for q in raw_quotes]\n",
        "\n",
        "    mapped_quotes = map_quotes_to_times(raw_quotes, data[\"segments\"], max_quotes=4)\n",
        "    summary[\"zitate\"] = mapped_quotes\n",
        "\n",
        "    # Markdown + Save\n",
        "    md_text = render_markdown(summary, url=youtube_url, title=data[\"title\"])\n",
        "    out_path = save_markdown(md_text, data[\"title\"])\n",
        "    print(f\"\\n‚úÖ Fertig. Datei gespeichert: {out_path}\")\n",
        "    return out_path\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# Test\n",
        "# ==============================\n",
        "TEST_URL = \"https://www.youtube.com/watch?v=yN2iYWbwFFs\"\n",
        "run_pipeline(TEST_URL)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9UzqdnBe26di"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
